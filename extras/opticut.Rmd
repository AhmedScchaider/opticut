---
title: 'opticut: likelihood based optimal partitioning for indicator species analysis'
author: Peter Solymos, Ermias T. Azeria, Bela Tothmeresz
date: "November 17, 2015"
output: pdf_document
---

# Introduction

General problem: find where species abundances are high vs. low in
a way which leads to optimal classification by maximizing the
contrast between the partitions.

Previous attempts: historical review, highlighting IndVal.

Issues with previous attempts:

* summary statistics & Monte Carlo randomization, no model,
* data types not always compatible with randomization (i.e. decimals),
* confounding effects to classification accuracy can impact power.

Goals: 

* describe a general and extensible approach,
* implement a computationally efficient algorithm,
* tools for exploring the results (i.e. summaries, plots) in a object oriented framework.

# Theory

## The quest for optimal binary partitioning

$Y_{i}$'s are observations for a single species
from $n$ locations ($i = 1, ..., n$).
$g_{i}$'s are known discrete descriptors of the locations with
$K$ levels ($K > 2$). $z^{(m)}$ is a binary reclassification of $g$ taking
values (0, 1). The superscript $m = 1, ..., M$ indicates a possible combination of binary reclassification out of the total $M = 2^{K-1} - 1$ total combinations (excluding complements). See below for options for defining binary partitions.
There can also be other site descriptors denoted as $x_{ij}$
taking discrete or continuous values ($j = 1, ..., p$; number of predictors).

A suitable parametric model describe the
relationship between the observations and the site decriptors
through the probability density function 
$P(Y_{i} = y_{i} \mid z_{i}^{(m)}, x_{ij}, \theta)$ 
where $\theta$ is the vector of model parameters: 
$\theta = (\beta_{0}, \beta_{1}, \alpha_{1}, ..., \alpha_{p})$.
The choice of the parametric model depends on the nature of the 
observations. It can be Gaussian, Binomial, Poisson,
ordinal, Beta regression, or zero-inflated models, with a 
suitable link function ($f$) for the mean:
$f(\eta_{i}) = \beta_{0}^{(m)} + \beta_{1}^{(m)} z_{i}^{(m)} + \sum_{j=1}^{p} \alpha_{j}^{(m)} x_{ij}$.

$\widehat{\theta^{(m)}}$ is the maximum likelihood estimate (MLE) of the 
model parameters given the data and classification $m$, 
with corresponding log-likelihood value $l(\widehat{\theta^{(m)}}; y)$.
Finding MLEs for all $M$ candidate binary partitions
leads to a set of log-likelihood values. One can compare
the log-likelihood values to a null model (no binary partition is necessary)
where $\beta_{1} = 0$ leading to the MLE $\widehat{\theta^{(0)}}$
and corresponding log-likelihood value for the null model: 
$l(\widehat{\theta^{(0)}}; y)$.

The log-likelihood ratio for each cadidate partition can be
calculated as $l(\widehat{\theta^{(m)}}; y) - l(\widehat{\theta^{(0)}}; y)$.
The best supported binary partition is
the model with the highest log-likelihood ratio value.

The indicator value ($I$) for each candidate partition can be calculated
based on expected values using the inverse link function as 
$\mu_{0}^{(m)} = f^{-1}(\beta_{0}^{(m)})$ and 
$\mu_{1}^{(m)} = f^{-1}(\beta_{0}^{(m)} + \beta_{1}^{(m)})$.
$I = 1 - min(\mu_{0}^{(m)}, \mu_{1}^{(m)}) / max(\mu_{0}^{(m)}, \mu_{1}^{(m)})$.
Where $\mu_{0}^{(m)} = E[Y_{i} \mid z_{i}^{(m)}=0, x_{ij}=0]$ and 
$\mu_{1}^{(m)} = E[Y_{i} \mid z_{i}^{(m)}=1, x_{ij}=0]$ are expected values
for the observations given the binary partition $z_{i}^{(m)}$
and at 0 value for all $x_{ij}$. 

## Finding all possible binary partitions

Finding all combinations does not require a model or observed responses.
It only takes a classification vector with $K > 1$ partitions.

`kComb` returns a 'contrast' matrix corresponding to
all possible binary partitions of the factor with `K` levels.
Complements are not counted twice, i.e.
(0,0,1,1) is equivalent to (1,1,0,0).
The number of such possible combinations is $M = 2^{K-1} - 1$.

```{r, echo=FALSE}
source("~/repos/opticut/R/opticut.R")
#options("digits"=7)
```
```{r}
kComb(k = 2)
kComb(k = 3)
kComb(k = 4)
```

`allComb` this takes a classification vector with at least 2 levels
and returns a model matrix with binary partitions. `checkComb`
checks if combinations are unique and non-complementary
(misfits are returned as attributes).

```{r}
(f <- rep(LETTERS[1:4], each=2))
(mc <- allComb(f, collapse = "_"))
checkComb(mc)
mc2 <- cbind(z = 1 - mc[,1], mc[,c(1:ncol(mc), 1)])
colnames(mc2) <- 1:ncol(mc2)
mc2
checkComb(mc2)
```

## Poisson count model example

```{r}
set.seed(1234)
n <- 200
x0 <- sample(1:4, n, TRUE)
x1 <- ifelse(x0 %in% 1:2, 1, 0)
x2 <- rnorm(n, 0.5, 1)
table(x0,x1)
lam1 <- exp(0.5 + 0.5*x1 + -0.2*x2)
boxplot(lam1~x0)
Y1 <- rpois(n, lam1)
lam2 <- exp(0.1 + 0.5*ifelse(x0==4,1,0) + 0.2*x2)
boxplot(lam2~x0)
Y2 <- rpois(n, lam2)
lam3 <- exp(0.1 + -0.2*x2)
boxplot(lam3~x0)
Y3 <- rpois(n, lam3)
Y <- cbind(SPP1=Y1, SPP2=Y2, SPP3=Y3)
X <- model.matrix(~x2)
Z <- allComb(x0)
opticut1(Y1, X, Z, dist="poisson")
opticut1(Y2, X, Z, dist="poisson")
opticut1(Y3, X, Z, dist="poisson")
summary(m <- opticut(Y ~ x2, strata=x0, dist="poisson", comb="all"))
plot(m, cut=-Inf)
```

Describe here what is what in the output.

## Not using all possible partitions

Blindly fitting a model to all possible partitions is wasteful
use of resources. Instead, one can rank the $K$ partitions
based on expected response values 
($\mu_{1}, ..., \mu_{k}, ..., \mu_{K}$, 
where $\mu_{k}=E[Y_{i} \mid g_{i}=k, x_{ij}=0]$).
This way we haveto explore only $K-1$ partitions:

```{r}
oComb(1:4)
```

`oComb` return the 'contrast' matrix based on the rank vector as input.
Rank 1 means lowest expected value among the partitions.

The function `rankComb` fits the model with multiple ($K > 2$) factor levels
to find out the ranking, and returns a binary classification matrix
similarly to `allComb`:

```{r}
head(rc <- rankComb(Y1, model.matrix(~x2), as.factor(x0), dist="poisson"))
attr(rc, "est")
```

Note that the ranking varies from species to species, thus
it is not possible to supply the resulting matrix as
`strata` definition:

```{r}
summary(opticut(Y ~ x2, strata=x0, dist="poisson", comb="rank"))
```

There is an overhead of fitting the model to calculate the ranking.
But computing efficiencies can be still high compared to all partitions
when the number of levels ($k$) is high.

# Distributions

Currently available distributions:

* `"gaussian"`: real valued continuous observations, e.g. biomass,
* `"poisson"`: Poisson count data,
* `"binomial"`: presence-absence type data,
* `"negbin"`: overdispersed Negative Binomial count data,
* `"beta"`: continuous response in the unit interval, e.g. percent cover,
* `"zip"`: zero-inflated Poisson counts,
* `"zinb"`: zero-inflated Negative Binomial counts,
* `"ordered"`: response measured on ordinal scale, e.g. ordinal vegetation cover, 
* `"rspf"`: presence-only data using resource selection probability functions.


## Gaussian

```{r}
Y <- rnorm(n, log(lam1) + 10, 0.5)
(mod <- opticut(Y ~ x2, strata=x0, dist="gaussian"))
```

Legendre example

```{r}
gr <- rep(1:5, each=5)
spp <- cbind(Species1=rep(c(4,6,5,3,2), each=5),
    Species2=c(rep(c(8,4,6), each=5), 4,4,2, rep(0,7)),
    Species3=rep(c(18,2,0,0,0), each=5))
rownames(spp) <- gr
spp
summary(mod <- opticut(spp ~ 1, strata=gr, dist="gaussian", comb="all"))
summary(mod <- opticut(spp ~ 1, strata=gr, dist="gaussian", comb="rank"))
```

## Binomial

```{r}
set.seed(1234)
n <- 1000
x0 <- sample(1:4, n, TRUE)
x1 <- ifelse(x0 %in% 1:2, 1, 0)
x2 <- rnorm(n, 0.5, 1)
table(x0,x1)
p1 <- plogis(0.5 + 0.5*x1 + -0.2*x2)
boxplot(p1~x0)
Y1 <- rbinom(n, 1, p1)
p2 <- plogis(0.1 + 0.5*ifelse(x0==4,1,0) + 0.2*x2)
boxplot(p2~x0)
Y2 <- rbinom(n, 1, p2)
Y <- cbind(SPP1=Y1, SPP2=Y2)
X <- model.matrix(~x2)
Z <- allComb(x0)

summary(opticut(Y ~ x2, strata=x0, dist="binomial"))
```


## Poisson: Mite data set -- high performance computing

See computing time diffs and plotting options.

```{r}
library(vegan)
data(mite)
data(mite.env)
mite.env$hab <- with(mite.env, interaction(Shrub, Topo, drop=TRUE))
summary(mod0 <- opticut(as.matrix(mite) ~ SubsDens, mite.env,
    strata=mite.env$hab, dist="poisson", comb="all"))
plot(mod0)

system.time(aa <- opticut(as.matrix(mite) ~ 1, strata=mite.env$hab, dist="poisson", comb="rank"))
system.time(bb <- opticut(as.matrix(mite) ~ 1, strata=mite.env$hab, dist="poisson", comb="all"))

## sequential
system.time(opticut(as.matrix(mite) ~ 1, strata=mite.env$hab, dist="poisson"))
## parallel -- compare system times
library(parallel)
cl <- makeCluster(3)
system.time(opticut(as.matrix(mite) ~ 1, strata=mite.env$hab, dist="poisson", cl=cl))
stopCluster(cl)
## forking -- will not work on Windows
system.time(opticut(as.matrix(mite) ~ 1, strata=mite.env$hab, dist="poisson", cl=3))
```

## Percenages

### Dune data, cover type data

See http://www.davidzeleny.net/anadat-r/doku.php/en:data:dune

```{r}
library(vegan)
data(dune)
data(dune.env)

## ordinal regr
## (when nlevels() < 3 use logistic regression instead !!!
Dune <- as.matrix(dune)
#Dune <- Dune[,apply(Dune, 2, function(z) length(unique(z)))>2]
x <- opticut(Dune ~ 1, strata=dune.env$Management, dist="ordered")
summary(x)

## Binarizing data
Dune <- ifelse(as.matrix(dune)>0,1,0)
x <- opticut(Dune ~ 1, strata=dune.env$Management, dist="binomial")
summary(x)

## Beta regression
Dune <- as.matrix(dune+0.5) / 10
x <- opticut(Dune ~ 1, strata=dune.env$Management, dist="beta")
summary(x)
```

### Stratigraphy example

```{r}
library(rioja)
data(aber)
strat.plot(aber$spec, aber$ages$Depth, scale.percent=TRUE, y.rev=TRUE)

z <- as.factor(cut(aber$ages$Depth, 5))
ab <- as.matrix(aber$spec) / 100
ab[ab == 0] <- 0.0001
ab <- ab[,apply(ab, 2, max) > 0.05]

a <- opticut(ab ~ 1, strata=z, comb="rank", dist="beta")
summary(a)
bp <- bestpart(a)

op <- par(mfrow=c(3,4), mar=c(2,2,1,1))
for (i in 1:12) {
    plot(ab[,i], aber$ages$Depth, type="l", ann=FALSE)
    segments(x0=rep(0, nrow(ab)), y0=aber$ages$Depth, x1=ab[,i], 
        col=ifelse(bp[,i] > 0, 2, 1))
    title(main=colnames(ab)[i])
}
par(op)
```


## Presence-only data

```{r}
## presence-only data
## single species model only:
## because the used distr is different for
## each species by definition.

library(ResourceSelection)
## settings
n.used <- 1000
m <- 10
n <- n.used * m
set.seed(1234)
x <- data.frame(x0=as.factor(sample(1:3, n, replace=TRUE)),
    x1=rnorm(n), x2=runif(n))
cfs <- c(1, -0.5, 0.1, -1, 0.5)
## Logistic RSPF model
dd <- simulateUsedAvail(x, cfs, n.used, m, link="logit")

Y <- dd$status
X <- model.matrix(~ x1 + x2, dd)
Z <- allComb(as.integer(dd$x0))

(mod <- opticut1(Y, X, dd$x0, dist="rspf", m=0, B=0))
```

# Custom distributions

The `distr` argument accepts a function, so other parametric models
can be supplied which are avoided due to package dependencies. 

## Mixed models

Here is an example using mixed models and the package `lme4`:

```{r}
library(lme4)
set.seed(1234)
n <- 200
x0 <- sample(1:4, n, TRUE)
x1 <- ifelse(x0 %in% 1:2, 1, 0)
x2 <- rnorm(n, 0.5, 1)
ee <- rnorm(n/5)
g <- rep(1:5, each=n/5)
lam1 <- exp(0.5 + 0.5*x1 + -0.2*x2 + ee[g])
Y1 <- rpois(n, lam1)

X <- model.matrix(~x2)
Z <- allComb(x0)

lmefun <- function(Y, X, linkinv, gr, ...) {
    X <- as.matrix(X)
    m <- glmer(Y ~ X-1 + (1|gr), family=poisson("log"), ...)
    list(coef=fixef(m),
        logLik=logLik(m),
        linkinv=family(m)$linkinv)
}
lmefun(Y1, X, gr=g)

opticut1(Y1, X, Z, dist=lmefun, gr=g)
```

## Imperfect detectability: N-mixture case

A single-visit based N-mixture is an example where
detection error is estimated. Let us compare results
based on naive GLM and N-mixture:

```{r}
library(detect)
set.seed(1234)
n <- 200
x0 <- sample(1:4, n, TRUE)
x1 <- ifelse(x0 %in% 1:2, 1, 0)
x2 <- rnorm(n, 0.5, 1)
x3 <- runif(n, 0, 1)
lam <- exp(0.5 + 1*x1 + -0.2*x2)
p <- plogis(2 + -2*x3)
Y <- rpois(n, lam*p)

X <- model.matrix(~x2)

op <- par(mfrow=c(1,2))
boxplot((lam*p) ~ x0, ylab="lam*p", xlab="x0")
boxplot(lam ~ x0, ylab="lam", xlab="x0")
par(op)

svfun <- function(Y, X, linkinv, ...) {
    X <- as.matrix(X)
    m <- svabu(Y ~ X-1 | x3, ...)
    list(coef=coef(m, "sta"),
        logLik=logLik(m),
        linkinv=poisson()$linkinv)
}
svfun(Y, X)

## naive GLM
print(opticut1(Y, X, as.factor(x0), dist="poisson"), cut=-Inf)
## N-mixture
print(opticut1(Y, X, as.factor(x0), dist=svfun), cut=-Inf)
```

## Sampling error differences: using offsets

Not accounting for unequal sampling effort can be quite misleading, especially
if that is related to habitat classes. This example shows how to take
advantage of the other arguments passed to the `...` in the `opticut`
function.

```{r}
set.seed(1234)
n <- 50
x0 <- sample(1:4, n, TRUE)
x1 <- ifelse(x0 %in% 1:2, 1, 0)
x2 <- rnorm(n, 0.5, 1)
lam <- exp(0.5 + 1*x1 + -0.2*x2)
A <- ifelse(x0 %in% c(1,3), 1, 2)
Y <- rpois(n, lam*A)

op <- par(mfrow=c(1,2))
boxplot((lam*A) ~ x0, ylab="lam*A", xlab="x0")
boxplot(lam ~ x0, ylab="lam", xlab="x0")
par(op)

## no offset: incorrect
opticut(Y ~ x2, strata=x0, dist="poisson", comb="rank")$species
## with offsets: log Area
opticut(Y ~ x2, strata=x0, dist="poisson", offset=log(A), comb="rank")$species
```

# Finding best partitions

It is useful to access the best binary partition

```{r}
set.seed(2345)
n <- 50
x0 <- sample(1:4, n, TRUE)
x1 <- ifelse(x0 %in% 1:2, 1, 0)
x2 <- rnorm(n, 0.5, 1)
lam <- exp(0.5 + 1*x1 + -0.2*x2)
Y <- rpois(n, lam)
o <- opticut(Y ~ x2, strata=x0, dist="poisson", comb="rank")
summary(o)

bp <- bestpart(o)
head(bp)
```

The model based on the best partition can be returned as:

```{r}
bestmodel(o, which=1)
```

the `which` argument can be used to subset the species.

# Uncertainty

Uncertainty in $I$ values might be of interest.
The `type` argument for the `uncertainty` method can take the following values:

* `"asymp"`: asymptotic distribution of $I$, $\mu_{0}$ and $\mu_{1}$
  based on best partition found for the input object.
* `"boot"`: non-parametric bootstrap distribution of $I$, $\mu_{0}$ and $\mu_{1}$
  based on best partition found for the input object.
* `"multi"`: non-parametric bootstrap distribution of $I$, $\mu_{0}$ and $\mu_{1}$
  based on best partition found for the bootstrap data (i.e.
  the model ranking is re-evaluated each time).

```{r}
uc1 <- uncertainty(o, 1, type="asymp", B=5000)
uc2 <- uncertainty(o, 1, type="boot", B=200)
uc3 <- uncertainty(o, 1, type="multi", B=200)

summary(uc1[[1]])
summary(uc2[[1]])
summary(uc3[[1]])

plot(density(uc1[[1]]$I), ylim=c(0,7))
lines(density(uc2[[1]]$I), col=2)
lines(density(uc3[[1]]$I), col=4)
legend("topleft", lty=1, col=c(1,2,4),
    legend=c("asymp", "boot", "multi"))
```

